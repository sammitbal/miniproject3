---
title: "MiniProject3"
author: "Sammit Bal, Jason Zheng, Thaddeus"
date: "2025-04-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Frontmatter

```{r}
#remove(list = ls())
# All Libraries
library(glmnet)
library(rpart)
library(rattle) # for the fancyRpartPlot()
library(tidyverse)
library(readxl)
library(gridExtra)
# The Dataset
COD_dataset <- read_excel("./CODGames2_mp.xlsx") 

# WinLoss = 1 (Win), 0 (Loss)
COD_dataset <- COD_dataset %>%
  # Creating integers PlayerScore and OtherScore
  separate(Result, 
           into = c("PlayerScore", "OtherScore"), 
           sep = "-") %>%
  mutate(PlayerScore = as.integer(PlayerScore)) %>%
  mutate(OtherScore = as.integer(OtherScore)) %>%
  # Restoring "Result" variable
  mutate(Result = COD_dataset$Result) %>%
  mutate(WinLoss = ifelse(PlayerScore > OtherScore, "Win", "Loss")) %>%
  mutate(WinLossNum = as.factor(ifelse(WinLoss == "Win", 1, 0)))
```


## Task 1: TotalXP + XPType (Thaddeus)

After removing the cases in which the player only participated in a fraction of the match, create side-by-side boxplots showing the relationship between TotalXP and XPType. (Be sure to use proper axis labels rather than the variable names.) Supplement the plots with summary statistics of TotalXP for each level of XPType. What have you learned about the relationship between XPType and TotalXP?


```{r}
COD_dataset %>%
  filter(FullPartial == "Full")
```



```{r}  
xpType <-ggplot(COD_dataset, aes(x = XPType, 
                        y = TotalXP, 
                        color = XPType)) +
  labs(x = "Type of XP earned", 
       y = "Total XP earned in a game", 
       title = "Relationship between TotalXP and XPType") +
geom_boxplot()

xpType
```


```{r}
COD_dataset %>%
  group_by(XPType) %>%
  summarise(
    mean_TotalXP = mean(TotalXP, na.rm = TRUE),
    median_TotalXP = median(TotalXP, na.rm = TRUE),
    sd_TotalXP = sd(TotalXP, na.rm = TRUE),
    count = n()
  )
```

Based off the graph, Double XP + 10% XP boost have a significantly higher median than just a 10% XP boost. To be exact, the average difference exceeds 7,000 total XP, with the difference of means being 8,284 and median d difference being 7,108 XP to be exact. That's a 94.6% increase in mean, or 82.6% increase in median. 

*Note:* Notably, the reason why the mean difference is notably skewed upwards is that there are far more extreme outliers for Double XP + 10%, meaning that while the relative averages for 10% versus Double XP + 10% boost are roughly the same for mean and median, there's a 1.4k XP skew for Double XP + 10% because of a notable difference in the number of outlier data points in that category. 



## Task 2

Suppose we wish to build an appropriate model for modeling the Score variable for games in which the player participated in the full match of a HC – TDM game type. We wish to answer the following research question: Of the predictors total XP, eliminations, deaths, damage, XPType, and whether the player’s team won, which should be included in a model for the Score? To answer this, you will have to create a new variable that indicates whether the player was on the winning team or not. NOTE: Since this is an inference question and we are not worried about how well the model will generalize to new data, there is no need to do a training/validation split in this problem.

### Part (a) (Sammit)

- Implement LASSO regression and one other feature selection procedure that we covered in Lecture 15. 
- Include relevant plots, a discussion on which value of lambda you selected, the estimated equation from LASSO and the estimated equation from the second method. 
- Discuss/compare the results of LASSO with those of the other method. 


```{r}
# Selecting for HC-TDM + Full 
filtered_data <- COD_dataset %>%
  filter(GameType == "HC - TDM", FullPartial == "Full")

filtered_data <- filtered_data %>%
    mutate(WinLossNum = as.factor(ifelse(WinLoss == "Win", 1, 0)))

# Setting up for LASSO CV
Xmat <- model.matrix(Score ~ TotalXP + Eliminations + Deaths + Damage + XPType + WinLossNum, data = filtered_data)[, -1]
yvec <- filtered_data$Score

# Performing LASSO CV
set.seed(123)
lassoCV <- cv.glmnet(x = Xmat, y = yvec,
                     family = "gaussian",
                     alpha = 1, 
                     lambda = NULL,
                     standardize = TRUE,
                     nfolds = 10)
set.seed(NULL)

plot(lassoCV) # View the result!
```

```{r}
### Selecting which lambda value

lassoCV$lambda.min
lassoCV$lambda.1se

# Store the coefficients associated with the optimal values
coefLamMin <- predict(lassoCV, s = lassoCV$lambda.min, type = "coefficients")
coefLam1se <- predict(lassoCV, s = lassoCV$lambda.1se, type = "coefficients")

# Create a data frame for comparing the coefficients
tempdf <- 
  data.frame(Variable = row.names(coefLamMin), 
             lamMin = as.numeric(coefLamMin), 
             lam1se = as.numeric(coefLam1se))

tempdf
```

```{r}
# Evaluating performance of the model (RMSE)
lassoYhat <- predict(lassoCV,
                     s = lassoCV$lambda.min,
                     newx = Xmat)
lassoMSE <- mean((yvec - lassoYhat)^2)
lassoRMSE <- sqrt(lassoMSE)
lassoRMSE 
```



```{r}
### f Model: Backward elimination
int_only_model <- lm(Score ~ 1, 
                     family = binomial, 
                     data = filtered_data)

# Full Model -> Backwards Seelction
full_model <- lm(Score ~ TotalXP + Eliminations + Deaths + Damage + XPType + WinLossNum, 
                  family = binomial, 
                  data = filtered_data)

stats::step(object = full_model, 
            scope = list(lower = int_only_model, upper = full_model),
            data = filtered_data,
            direction = "backward")

# Calculating RMSE
rss <- sum(residuals(full_model)^2)
n <- nobs(full_model)

rmse <- sqrt(rss / n)
rmse
```

**Discussion:**

The estimated equation for Lasso is: $\hat(y) = 937.15693473 + 0.05984054x_(TotalXP) + 159.12937918x_(Eliminations) + -72.77820466x_(Deaths) + 0.94772672x_(Damage) + -361.57550500x_(XPTypeDouble XP) + -447.19199686x_(WinLossNum1)$

The estimated equation for Backward Elimination is: $\hat(y) = 944.05302 + 0.06054x_(TotalXP) + 185.24081x_(Eliminations) + -73.25104x_(Deaths) + -367.94000x_(XPTypeDouble XP) + -454.44701x_(WinLossNum1)$

The lambda I selected was 2.611974 which came from the min lambda. I chose this because it resulted in a better RMSE value. 

Looking at the two RMSE values from these methods Backward Elimination is slightly better. Backward Elimination has an RMSE of 692.8435, while LASSO has an RMSE of 692.9045. 

Looking at the variables both methods selected is also interesting. Both methods kept TotalXP, Eliminations, Deaths, XPTypeDouble XP, and WinLossNum1. The one variable the left out in Backward Elimination was damage. This is expected behavior as LASSO tends to shrink small coefficients like Damage without fully dropping them. While Backward Elimination might drop variables that don’t improve the model.


### Part (b) (Jason)

- Build a regression tree for predicting Score using total XP, eliminations, deaths, damage, XPType, and whether the player’s team won. 
- Specify that each node must contain at least 15 observations. 
- Display the tree and report the variables associated with the 3 highest variable importance values. (Include the variable importance values when mentioning the variables.)

```{r}
# Creating + Visualizing the regression tree
RegTree <- rpart(Score ~ TotalXP + Eliminations + Deaths + Damage + XPType + WinLossNum,
                 method = "anova", # Regression Tree
                 data = COD_dataset,
                 minbucket = 15) # Each node >= 15 observations

fancyRpartPlot(RegTree, cex = 0.7)

# Three Most Important Variables
varImps <- 100*RegTree$variable.importance/sum(RegTree$variable.importance)
varImps
```

**Variable Importances:** In the context of a Regression Tree built upon the variables TotalXP, Eliminations, Deaths, Damage, XPType, and WinLossNum (an indicator variable, with 1 meaning the team won, and 0 if they drew or lost), the three most important variables for creating the tree were Damage (44.22%-important to the tree's creation), Elimination (41.25%-important to the tree's creation), and TotalXP (11.90%-important to the tree's creation). 

### Part (c) (Jason)

When building linear regression models, we often wish to determine which variables are the most important. One way of doing this is to look at the magnitude (absolute value) of the estimated coefficients for the regression model built using standardized inputs (centered to have a mean of 0 and a standard deviation of 1). 

*Based on the variables selected by the other feature selection procedure from part (a)* (in other words, not the LASSO model)...

1. standardize the inputs, 
2. build the regression model, 
3. report the estimated equation, and
4. report the 3 most important variables based on the magnitude (absolute value) of the estimated coefficients. 

**Question:** How does this compare to the most important variables based on the regression tree?

```{r}
### Considering the variables from ALT METHOD (backwards selection)
# Score ~ TotalXP + Eliminations + Deaths + Damage + XPType + WinLossNum
# scaled_filtered_data <- filtered_data %>%
#   select(c("Score", "TotalXP", "Eliminations", "Deaths", "Damage", "XPType", "WinLossNum")) %>%
#   mutate(XPType = )
# 
# View(scaled_filtered_data)
# mean(scaled_filtered_data[0])
# 
# ### f Model: Backward elimination
# int_only_model <- lm(Score ~ 1, 
#                      family = binomial, 
#                      data = filtered_data)
# 
# # Full Model -> Backwards Seelction
# full_model <- lm(Score ~ TotalXP + Eliminations + Deaths + Damage + XPType + WinLossNum, 
#                   family = binomial, 
#                   data = filtered_data)
# 
# # Standardize the inputs
# scaled_int
# 
# stats::step(object = full_model, 
#             scope = list(lower = int_only_model, upper = full_model),
#             data = filtered_data,
#             direction = "backward")
# 
# # Calculating RMSE
# rss <- sum(residuals(full_model)^2)
# n <- nobs(full_model)
# 
# rmse <- sqrt(rss / n)
# rmse
```



